{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44106bd3",
   "metadata": {},
   "source": [
    "# Build EN->RU CEFR Corpus\n",
    "\n",
    "This notebook loads the CEFR-labelled English texts, translates each entry into Russian using the Hugging Face Inference API, and stores the combined dataset as `data/text/en_ru_cefr_corpus.csv`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "621f08a9",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "- Install the required packages (`pandas`, `requests`, optionally `python-dotenv` for local secrets).\n",
    "- Create a Hugging Face account and generate an [Inference API token](https://huggingface.co/settings/tokens) with access to public models.\n",
    "- Expose the token to the notebook via the `HF_API_TOKEN` environment variable (for example, in Jupyter run `'%env HF_API_TOKEN=hf_xxx'`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0698e32c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "from pathlib import Path\n",
    "from typing import Sequence\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "\n",
    "SOURCE_CSV = Path(\"../data/text/cefr_leveled_texts.csv\")\n",
    "OUTPUT_CSV = Path(\"../data/text/en_ru_cefr_corpus.csv\")\n",
    "MODEL_URL = \"https://api-inference.huggingface.co/models/Helsinki-NLP/opus-mt-en-ru\"\n",
    "BATCH_SIZE = 3\n",
    "REQUEST_TIMEOUT = 60\n",
    "RETRY_LIMIT = 6\n",
    "\n",
    "HF_API_TOKEN = \"\"\n",
    "# Optionally override in-place: HF_API_TOKEN = \"hf_your_api_token\"  # noqa: E605\n",
    "\n",
    "if HF_API_TOKEN is None:\n",
    "    raise RuntimeError(\n",
    "        \"Set HF_API_TOKEN environment variable (or assign in this cell) with your Hugging Face Inference token.\"\n",
    "    )\n",
    "\n",
    "if not SOURCE_CSV.exists():\n",
    "    raise FileNotFoundError(f\"Missing source corpus at {SOURCE_CSV.resolve()}\")\n",
    "\n",
    "OUTPUT_CSV.parent.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "544c4bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HuggingFaceTranslator:\n",
    "    \"\"\"Thin wrapper around the Hugging Face inference endpoint for EN->RU translation.\"\"\"\n",
    "\n",
    "    def __init__(self, endpoint: str, token: str, *, timeout: int = 60, sleep: float = 1.0) -> None:\n",
    "        if not token:\n",
    "            raise ValueError(\"Token must be provided for the Hugging Face Inference API.\")\n",
    "        self.endpoint = endpoint\n",
    "        self.headers = {\"Authorization\": f\"Bearer {token.strip()}\"}\n",
    "        self.timeout = timeout\n",
    "        self.sleep = max(sleep, 0.0)\n",
    "        self._session = requests.Session()\n",
    "\n",
    "    def translate_batch(self, texts: Sequence[str], *, retries: int = 6) -> list[str]:\n",
    "        if not texts:\n",
    "            return []\n",
    "        payload = {\n",
    "            \"inputs\": list(texts),\n",
    "            \"parameters\": {\"max_length\": 1024},\n",
    "            \"options\": {\"wait_for_model\": True},\n",
    "        }\n",
    "        last_error: Exception | None = None\n",
    "        for attempt in range(retries):\n",
    "            try:\n",
    "                response = self._session.post(\n",
    "                    self.endpoint,\n",
    "                    headers=self.headers,\n",
    "                    json=payload,\n",
    "                    timeout=self.timeout,\n",
    "                )\n",
    "            except requests.RequestException as exc:  # network hiccup\n",
    "                last_error = exc\n",
    "            else:\n",
    "                if response.status_code == requests.codes.ok:\n",
    "                    data = response.json()\n",
    "                    translations = self._extract_translations(data, expected=len(texts))\n",
    "                    if self.sleep:\n",
    "                        time.sleep(self.sleep)\n",
    "                    return translations\n",
    "                if response.status_code in {requests.codes.service_unavailable, requests.codes.too_many_requests}:\n",
    "                    wait_time = self.sleep or 1.0\n",
    "                    wait_time *= 2 ** attempt\n",
    "                    time.sleep(min(wait_time, 30))\n",
    "                    continue\n",
    "                try:\n",
    "                    details = response.json()\n",
    "                except ValueError:\n",
    "                    details = response.text\n",
    "                raise RuntimeError(f\"Translation failed: {response.status_code} -> {details}\")\n",
    "        raise RuntimeError(\"Repeated translation failures\") from last_error\n",
    "\n",
    "    @staticmethod\n",
    "    def _extract_translations(data: object, *, expected: int) -> list[str]:\n",
    "        if isinstance(data, dict):\n",
    "            if \"error\" in data:\n",
    "                raise RuntimeError(f\"Model error: {data['error']}\")\n",
    "            if \"translation_text\" in data:\n",
    "                data = [data]\n",
    "        if isinstance(data, list) and data and isinstance(data[0], list):\n",
    "            data = data[0]\n",
    "        if not isinstance(data, list):\n",
    "            raise RuntimeError(f\"Unexpected response payload: {data}\")\n",
    "        translations: list[str] = []\n",
    "        for item in data:\n",
    "            if isinstance(item, dict) and \"translation_text\" in item:\n",
    "                translations.append(item[\"translation_text\"])\n",
    "            else:\n",
    "                raise RuntimeError(f\"Unexpected item in response: {item}\")\n",
    "        if len(translations) != expected:\n",
    "            raise RuntimeError(\n",
    "                f\"Expected {expected} translations but received {len(translations)}.\"\n",
    "            )\n",
    "        return translations\n",
    "\n",
    "\n",
    "MAX_SEGMENT_CHARS = 700\n",
    "\n",
    "\n",
    "def _split_paragraph(para: str, limit: int = MAX_SEGMENT_CHARS) -> list[str]:\n",
    "    para = \" \".join(para.strip().split())\n",
    "    if not para:\n",
    "        return []\n",
    "    if len(para) <= limit:\n",
    "        return [para]\n",
    "    sentences = re.split(r'(?<=[.!?])\\s+', para)\n",
    "    segments: list[str] = []\n",
    "    buffer = \"\"\n",
    "    for sentence in sentences:\n",
    "        sentence = sentence.strip()\n",
    "        if not sentence:\n",
    "            continue\n",
    "        if len(sentence) > limit:\n",
    "            if buffer:\n",
    "                segments.append(buffer.strip())\n",
    "                buffer = \"\"\n",
    "            for start in range(0, len(sentence), limit):\n",
    "                chunk = sentence[start : start + limit].strip()\n",
    "                if chunk:\n",
    "                    segments.append(chunk)\n",
    "            continue\n",
    "        next_len = len(buffer) + len(sentence) + (1 if buffer else 0)\n",
    "        if next_len <= limit:\n",
    "            buffer = f\"{buffer} {sentence}\".strip()\n",
    "        else:\n",
    "            if buffer:\n",
    "                segments.append(buffer.strip())\n",
    "            buffer = sentence\n",
    "    if buffer:\n",
    "        segments.append(buffer.strip())\n",
    "    return segments\n",
    "\n",
    "\n",
    "def split_text_for_api(text: str, limit: int = MAX_SEGMENT_CHARS) -> list[str]:\n",
    "    if not isinstance(text, str):\n",
    "        text = \"\" if text is None else str(text)\n",
    "    text = text.strip()\n",
    "    if not text:\n",
    "        return []\n",
    "    if len(text) <= limit:\n",
    "        return [text]\n",
    "    chunks: list[str] = []\n",
    "    paragraphs = [para for para in re.split(r\"\\n{2,}\", text) if para and para.strip()]\n",
    "    if not paragraphs:\n",
    "        paragraphs = [text]\n",
    "    for paragraph in paragraphs:\n",
    "        if len(paragraph) <= limit:\n",
    "            cleaned = \" \".join(paragraph.strip().split())\n",
    "            if cleaned:\n",
    "                chunks.append(cleaned)\n",
    "        else:\n",
    "            pieces = _split_paragraph(paragraph, limit=limit)\n",
    "            chunks.extend(pieces)\n",
    "    if not chunks:\n",
    "        return [text[:limit]]\n",
    "    return chunks\n",
    "\n",
    "\n",
    "def translate_text_safe(\n",
    "    text: str,\n",
    "    translator: HuggingFaceTranslator,\n",
    "    *,\n",
    "    batch_size: int = BATCH_SIZE,\n",
    "    retries: int = RETRY_LIMIT,\n",
    ") -> str:\n",
    "    segments = split_text_for_api(text)\n",
    "    if not segments:\n",
    "        return \"\"\n",
    "    outputs: list[str] = []\n",
    "    for start in range(0, len(segments), batch_size):\n",
    "        batch = segments[start : start + batch_size]\n",
    "        outputs.extend(translator.translate_batch(batch, retries=retries))\n",
    "    return \"\\n\\n\".join(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ccc54676",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1494 labelled texts.\n",
      "No existing translations found; starting from scratch.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row_id</th>\n",
       "      <th>text_en</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Hi!\\nI've been meaning to write for ages and f...</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>﻿It was not so much how hard people found the ...</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Keith recently came back from a trip to Chicag...</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>The Griffith Observatory is a planetarium, and...</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>-LRB- The Hollywood Reporter -RRB- It's offici...</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   row_id                                            text_en label\n",
       "0       0  Hi!\\nI've been meaning to write for ages and f...    B2\n",
       "1       1  ﻿It was not so much how hard people found the ...    B2\n",
       "2       2  Keith recently came back from a trip to Chicag...    B2\n",
       "3       3  The Griffith Observatory is a planetarium, and...    B2\n",
       "4       4  -LRB- The Hollywood Reporter -RRB- It's offici...    B2"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source_df = pd.read_csv(SOURCE_CSV).reset_index().rename(columns={\"index\": \"row_id\"})\n",
    "source_df = source_df.rename(columns={\"text\": \"text_en\"})\n",
    "required_columns = {\"row_id\", \"text_en\", \"label\"}\n",
    "missing = required_columns - set(source_df.columns)\n",
    "if missing:\n",
    "    raise ValueError(f\"Missing required columns: {sorted(missing)}\")\n",
    "source_df = source_df[[\"row_id\", \"text_en\", \"label\"]]\n",
    "source_df = source_df.sort_values(\"row_id\").reset_index(drop=True)\n",
    "print(f\"Loaded {len(source_df)} labelled texts.\")\n",
    "\n",
    "if OUTPUT_CSV.exists():\n",
    "    translated_df = pd.read_csv(OUTPUT_CSV)\n",
    "    if \"row_id\" in translated_df.columns:\n",
    "        processed_ids = set(\n",
    "            pd.to_numeric(translated_df[\"row_id\"], errors=\"coerce\").dropna().astype(int).tolist()\n",
    "        )\n",
    "    else:\n",
    "        processed_ids = set()\n",
    "    remaining_df = source_df[~source_df[\"row_id\"].isin(processed_ids)].copy()\n",
    "    print(\n",
    "        f\"Found existing translations for {len(processed_ids)} rows; \"\n",
    "        f\"{len(remaining_df)} still pending.\"\n",
    "    )\n",
    "else:\n",
    "    remaining_df = source_df.copy()\n",
    "    print(\"No existing translations found; starting from scratch.\")\n",
    "\n",
    "remaining_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfc59050",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translated rows 0-2 (3/1494 in this session).\n",
      "Translated rows 3-5 (6/1494 in this session).\n",
      "Translated rows 6-8 (9/1494 in this session).\n",
      "Translated rows 9-11 (12/1494 in this session).\n",
      "Translated rows 12-14 (15/1494 in this session).\n",
      "Translated rows 15-17 (18/1494 in this session).\n",
      "Translated rows 18-20 (21/1494 in this session).\n"
     ]
    }
   ],
   "source": [
    "translator = HuggingFaceTranslator(\n",
    "    MODEL_URL,\n",
    "    HF_API_TOKEN,\n",
    "    timeout=REQUEST_TIMEOUT,\n",
    "    sleep=1.2,\n",
    ")\n",
    "\n",
    "if remaining_df.empty:\n",
    "    print(\"All rows already translated.\")\n",
    "else:\n",
    "    write_header = not OUTPUT_CSV.exists()\n",
    "    total_pending = len(remaining_df)\n",
    "    for start in range(0, total_pending, BATCH_SIZE):\n",
    "        batch = remaining_df.iloc[start : start + BATCH_SIZE]\n",
    "        english_texts = batch[\"text_en\"].tolist()\n",
    "        row_ids = batch[\"row_id\"].tolist()\n",
    "        translations: list[str] = []\n",
    "        for row_id, text in zip(row_ids, english_texts):\n",
    "            text_value = \"\" if pd.isna(text) else str(text)\n",
    "            try:\n",
    "                translated_text = translate_text_safe(\n",
    "                    text_value,\n",
    "                    translator,\n",
    "                    batch_size=BATCH_SIZE,\n",
    "                    retries=RETRY_LIMIT,\n",
    "                )\n",
    "            except RuntimeError as exc:\n",
    "                print(f\"Failed to translate row {row_id}: {exc}\")\n",
    "                raise\n",
    "            translations.append(translated_text)\n",
    "        batch_out = pd.DataFrame(\n",
    "            {\n",
    "                \"row_id\": row_ids,\n",
    "                \"text_en\": english_texts,\n",
    "                \"text_ru\": translations,\n",
    "                \"label\": batch[\"label\"].tolist(),\n",
    "            }\n",
    "        )\n",
    "        batch_out.to_csv(OUTPUT_CSV, mode=\"a\", header=write_header, index=False)\n",
    "        write_header = False\n",
    "        first_id = batch_out[\"row_id\"].iloc[0]\n",
    "        last_id = batch_out[\"row_id\"].iloc[-1]\n",
    "        completed = start + len(batch_out)\n",
    "        print(\n",
    "            f\"Translated rows {first_id}-{last_id} \"\n",
    "            f\"({completed}/{total_pending} in this session).\"\n",
    "        )\n",
    "        time.sleep(0.3)\n",
    "\n",
    "print(\"Done.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31eafd07",
   "metadata": {},
   "outputs": [],
   "source": [
    "if OUTPUT_CSV.exists():\n",
    "    preview_df = pd.read_csv(OUTPUT_CSV)\n",
    "    print(f\"Generated {len(preview_df)} translated rows so far.\")\n",
    "    preview_df.head()\n",
    "else:\n",
    "    raise FileNotFoundError(\"Translation output not found; run the previous cell first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a065a311",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kazakh_cefr_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
